import asyncio
import pandas as pd
from typing import List, TypedDict
from langchain_ollama import ChatOllama, OllamaEmbeddings
from langchain_community.document_loaders import DataFrameLoader
from langchain_chroma import Chroma 
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langgraph.graph import START, StateGraph
from dotenv import load_dotenv

load_dotenv()

# --- 1. DATA INGESTION ---
excel_file = "backend/data/eng-products-export.xlsx" 
df = pd.read_excel(excel_file)
df['search_text'] = df.apply(
    lambda x: f"Product: {x['product name']} | Price: {x['price rec']} | PZN: {x['pzn']} | Size: {x['package size']} | Description: {x['descriptions']}", 
    axis=1
)

loader = DataFrameLoader(df, page_content_column="search_text")
docs = loader.load()
embeddings = OllamaEmbeddings(model="mxbai-embed-large")
vectorstore = Chroma.from_documents(documents=docs, embedding=embeddings)
retriever = vectorstore.as_retriever(search_kwargs={"k": 3})

# --- 2. AGENT STATE ---
class AgentState(TypedDict):
    question: str
    context: List[str]
    answer: str

# --- 3. WORKFLOW NODES ---
llm = ChatOllama(model="llama3.2", temperature=0)

async def retrieve_inventory(state: AgentState):
    # Standard invoke is fine here as retrieval is fast
    hits = await retriever.ainvoke(state["question"])
    return {"context": [h.page_content for h in hits]}

async def generate_pharmacist_response(state: AgentState):
    prompt = ChatPromptTemplate.from_template("""
    You are a professional Pharmacist. Use the retrieved inventory context to answer the user's question.
    Only recommend products found in the data below. Include prices.
    
    Inventory Data:
    {context}
    
    User Question: {question}
    
    Response:""")
    
    # We define the chain but call it using streaming in the execution block
    chain = prompt | llm | StrOutputParser()
    answer = await chain.ainvoke({"context": "\n".join(state["context"]), "question": state["question"]})
    return {"answer": answer}

# --- 4. GRAPH CONSTRUCTION ---
builder = StateGraph(AgentState)
builder.add_node("retrieve", retrieve_inventory)
builder.add_node("generate", generate_pharmacist_response)

builder.add_edge(START, "retrieve")
builder.add_edge("retrieve", "generate")

pharmacist_agent = builder.compile()

# --- 5. TOKEN STREAMING EXECUTION ---
async def start_chat():
    query = "I need something for well being"
    inputs = {"question": query}
    
    print(f"User: {query}")
    print("Pharmacist: ", end="", flush=True)

    # astream_events allows us to see the tokens as they are generated by the LLM
    async for event in pharmacist_agent.astream_events(inputs, version="v1"):
        kind = event["event"]
        
        # This event triggers for every new word/token from Llama 3.2
        if kind == "on_chat_model_stream":
            content = event["data"]["chunk"].content
            if content:
                print(content, end="", flush=True)

if __name__ == "__main__":
    try:
        asyncio.run(start_chat())
    except KeyboardInterrupt:
        pass